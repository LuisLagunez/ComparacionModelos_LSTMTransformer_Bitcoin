# Predicci√≥n del Precio de Bitcoin (BTC-USD) usando LSTM y Transformer con Walk-Forward

Este repositorio contiene un estudio completo sobre la predicci√≥n del precio de **Bitcoin (BTC-USD)** utilizando dos arquitecturas profundas:

- **LSTM (Long Short-Term Memory)**
- **Transformer Encoder**

Ambos modelos se entrenan mediante un esquema **Walk-Forward paralelo**, con recalibraci√≥n completa en cada ventana temporal, lo que simula un entorno real de predicci√≥n sin fuga de datos (*data leakage*).

Todo el proyecto fue desarrollado y ejecutado en **Google Colab**, por lo que se consideran tambi√©n sus limitantes (tiempo, RAM, GPU compartida, sesiones desconectadas, etc.).

---

## üìå Objetivos del proyecto

1. **Evaluar si los modelos secuenciales (LSTM y Transformer) pueden predecir el precio del Bitcoin utilizando el conjunto de variables OHLCV (Open, High, Low, Close, Volume).**
2. **Probar dos escenarios temporales:**
   - **Caso 2024:** Entrenamiento con 9 meses, predicci√≥n semanal (horizonte H=7).
   - **Caso Hist√≥rico (2010‚Äì2024):** Entrenamiento con 14 a√±os de datos, predicci√≥n mensual (horizonte H=30).
3. **Comparar ambas arquitecturas bajo el mismo esquema Walk-Forward.**
4. **Evitar completamente el data leakage**, usando:
   - Escaladores entrenados *solo* con datos previos.
   - Secuencias generadas correctamente hasta cada ventana.
   - Reentrenamiento modelo por ventana.

---

# üß† Arquitecturas utilizadas

### üîπ LSTM
- 2 capas
- 64 unidades
- Dropout 0.2
- Entrenamiento por ventana con Adam, LR=1e-3

### üîπ Transformer Encoder
- d_model = 64
- nheads = 4
- 2 capas encoder
- Activaci√≥n GELU
- AdamW, LR=1e-4
- Positional Encoding sinusoidal propio

Ambos modelos fueron entrenados entre **4 y 16 epochs por ventana**, con **repeticiones por ventana (ensembling)** para reducir la varianza de la predicci√≥n.

---

# üìä Esquemas temporales del estudio

## üü¶ Caso 1 ‚Äî A√±o 2024
- Datos: 1 ene 2024 ‚Üí 31 dic 2024  
- Entrenamiento: 1 ene ‚Üí 30 sep  
- Prueba: 1 oct ‚Üí 31 dic  
- Horizonte: **7 d√≠as (predicci√≥n semanal)**  
- Ventana: **60 d√≠as**

## üü© Caso 2 ‚Äî Historia completa (2010‚Äì2024)
- Datos: jul 2010 ‚Üí dic 2024  
- Entrenamiento: todo hasta dic 2023  
- Prueba: a√±o 2024  
- Horizonte: **30 d√≠as (predicci√≥n mensual)**  
- Ventana: **60 d√≠as**

---

# üîÑ Walk-Forward Rolling Training

Ambos modelos utilizan un esquema **realista y estricto**:

Para cada nuevo punto en el set de prueba:

1. Se toman **todas las secuencias anteriores como entrenamiento**.  
2. Se reescala √∫nicamente usando valores previos.  
3. Se entrena el modelo **desde cero** para esa ventana.  
4. Se predice solo 1 punto futuro.  
5. Se pasa a la siguiente ventana.

**Esto simula un trader o sistema automatizado real**, utilizando solo informaci√≥n disponible hasta ese momento.

---

# üß™ M√©tricas utilizadas

- **MAE** ‚Äî Error absoluto medio  
- **RMSE** ‚Äî Ra√≠z del error cuadr√°tico medio  
- **MAPE** ‚Äî Error porcentual absoluto medio  
- **sMAPE** ‚Äî Error porcentual sim√©trico  
- **Precisi√≥n promedio (%)**  
- **Precisi√≥n punto a punto (%)**  
- **Directional Accuracy (DA)**  
- **Correlaci√≥n de Pearson**

Todas las m√©tricas est√°n implementadas manualmente en el repositorio.

---

# üìÅ Estructura del c√≥digo

El c√≥digo puede visualizarse directamente [aqu√≠](Experimentacion_LSTMTransformer_Bitcoin.ipynb) o directamente en el notebook de [Google_Colab](https://colab.research.google.com/drive/1mSflyyC4mRUskUfOUhZFd-WnHKwKqr2x#scrollTo=Ph_Ts1AwjbuN).

https://colab.research.google.com/drive/1mSflyyC4mRUskUfOUhZFd-WnHKwKqr2x

El notebook contiene estos bloques principales:

### **Bloque 1 ‚Äî Descarga y limpieza de datos**
- Obtenci√≥n desde **Yahoo Finance** usando *yfinance*  
- Limpieza, orden temporal, splits y etiquetado

### **Bloque 2 ‚Äî Escalado y generaci√≥n de secuencias**
- MinMaxScaler entrenado **solo con entrenamiento**
- Secuencias multivariadas para cada horizonte
- Generador Walk-Forward parametrizable

### **Bloque 3 ‚Äî LSTM Walk-Forward**
- Implementaci√≥n desde cero en PyTorch
- Entrenamiento por ventana
- Ensemble de repeticiones

### **Bloque 4 ‚Äî Transformer Walk-Forward**
- Positional Encoding propio
- Encoder con GELU + LayerNorm
- AdamW y dropout
- Ensemble por ventana

### **Bloque 5 ‚Äî Evaluaci√≥n y visualizaci√≥n**
- Desescalado real
- C√°lculo de m√©tricas
- Gr√°ficos de comportamiento predictivo
- Exportaci√≥n de tabla consolidada

---

# üìà Resultados principales

El proyecto genera autom√°ticamente:

- M√©tricas comparativas completas en CSV.
- Gr√°ficos:
  - Serie Real vs LSTM vs Transformer.
  - Error absoluto comparado.
- Tabla final con todas las m√©tricas para los 4 escenarios:
  - LSTM 2024  
  - Transformer 2024  
  - LSTM Hist√≥rico  
  - Transformer Hist√≥rico  

**Los resultados pueden visualizarse directamente en Colab.**

---

# ‚öôÔ∏è Dependencias principales

```text
Python 3.10+
PyTorch
yfinance
scikit-learn
numpy
pandas
matplotlib
tqdm
scipy